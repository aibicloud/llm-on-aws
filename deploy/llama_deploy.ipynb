{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7060c891-cebd-4011-b350-b7d1e70b40b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy the pre-uploaded HuggingFace Model in S3 to Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa8a30-8a98-4b33-97a3-43fc8bf5a0dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step1: Initialize the Deploy Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4713d78-2418-4b09-9a18-5125a3d452ac",
   "metadata": {},
   "source": [
    "### 1.1 Install Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f413314-c410-43d3-bb3a-ba0aa18ec1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub -U -q -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install -U sagemaker -q -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install --upgrade sagemaker -q -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442aed5-14e4-48f7-9f8b-808fdc077853",
   "metadata": {},
   "source": [
    "### 1.2 Initialize Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be42fe3f-de11-47dc-a920-c4ea5e367c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b0a4b-f166-4f1a-a7cc-9c7277c68173",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the parameters for deployment\n",
    "- model_name ： HuggingFace中的模型名称 \n",
    "- s3_model_prefix ：模型文件在S3中的位置的文件夹路径（不包含bucket name和文件名称）【需要提前准备】\n",
    "- s3_code_prefix ： 模型执行代码在S3中的位置的文件夹路径（不包含bucket name和文件名称）【执行S3的文件夹路径即可，代码会自动上传到S3】\n",
    "- endpoint_config_name ： 部署Sagemaker Configuration 的名称\n",
    "- endpoint_name ： 部署Sagemaker endpoint的名称\n",
    "- deploy_cache_location ： 部署时，产生的代码文件所在的本地路径\n",
    "- inference_image_uri ： 部署所使用的推理容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be112a00-cbef-4387-b0d7-80e5e7b7030d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "s3_model_prefix = f\"llm/model/{model_name}\"\n",
    "s3_code_prefix = f\"llm/code/{model_name}\"\n",
    "deploy_cache_location = f\"../cache/{model_name}\"\n",
    "\n",
    "endpoint_model_name = f\"{model_name.replace('/', '-').replace('.', '-')}\"\n",
    "endpoint_config_name = endpoint_model_name # f\"{model_name}-config\"\n",
    "endpoint_name = endpoint_model_name\n",
    "\n",
    "\n",
    "inference_image_uri = f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.22.1-deepspeed0.8.3-cu118\"\n",
    "\n",
    "!mkdir -p $deploy_cache_location/code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f35a6f-5988-42ec-87b0-de36eaebe41b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Prepare code of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f080fc3-1c2e-4641-b62d-d0de26533256",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Prepare Model Entry Script："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70990dd3-431e-4dd0-a494-d26ceb454945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../cache/meta-llama/Llama-2-7b-chat-hf/code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $deploy_cache_location/code/model.py\n",
    "from djl_python import Input, Output\n",
    "from djl_python.streaming_utils import StreamingUtils\n",
    "import os\n",
    "import deepspeed\n",
    "import torch\n",
    "import logging\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "##\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import json\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "def get_model(properties):\n",
    "    model_name = properties[\"model_id\"]\n",
    "    tensor_parallel_degree = properties[\"tensor_parallel_degree\"]\n",
    "    max_tokens = int(properties.get(\"max_tokens\", \"768\"))\n",
    "    dtype = torch.float16\n",
    "\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, torch_dtype=dtype, device_map='auto')\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# system_prompt = \"\"\"\n",
    "# You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "#             If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "#             \"\"\"\n",
    "\n",
    "system_prompt = \"\"\n",
    "\n",
    "\n",
    "def get_prompt(message: str, chat_history: list[tuple[str, str]]) -> str:\n",
    "    texts = [f'[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    for user_input, response in chat_history:\n",
    "        texts.append(f'{user_input.strip()} [/INST] {response.strip()} </s><s> [INST] ')\n",
    "    texts.append(f'{message.strip()} [/INST]')\n",
    "    return ''.join(texts)\n",
    "\n",
    "\n",
    "def inference(inputs):\n",
    "    try:\n",
    "        input_map = inputs.get_as_json()\n",
    "        data = input_map.pop(\"ask\", input_map)\n",
    "        \n",
    "        if data.startswith(\"[INST]\"):\n",
    "            data = data\n",
    "        else:\n",
    "            data = get_prompt(data, [])\n",
    "        \n",
    "        parameters = input_map.pop(\"parameters\", {})\n",
    "        outputs = Output()\n",
    "\n",
    "        enable_streaming = inputs.get_properties().get(\"enable_streaming\",\n",
    "                            \"false\").lower() == \"true\"\n",
    "        if enable_streaming:\n",
    "            stream_generator = StreamingUtils.get_stream_generator(\n",
    "                \"DeepSpeed\")\n",
    "            outputs.add_stream_content(\n",
    "                stream_generator(model, tokenizer, data,\n",
    "                                 **parameters))\n",
    "            return outputs\n",
    "\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        tokenizer.padding_side = 'left'\n",
    "        input_tokens = tokenizer(data, padding=True,\n",
    "                                return_tensors=\"pt\").to(\n",
    "                                torch.cuda.current_device())\n",
    "        # with torch.no_grad():\n",
    "        #     output_tokens = model.generate(input_tokens.input_ids, **parameters)\n",
    "        # output_tokens = model.generate(input_tokens.input_ids, **parameters)\n",
    "        \n",
    "        # input_tokens = tokenizer(data, return_tensors='pt')\n",
    "        output_tokens = model.generate(input_tokens.input_ids, **parameters)\n",
    "\n",
    "        \n",
    "        # print(\"output_tokens\", json.dumps(output_tokens))\n",
    "        generated_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "\n",
    "        answer = [{\"generated_text\": s} for s in generated_text]\n",
    "        answer_text = ''\n",
    "        for item in answer:  \n",
    "            if '[/INST]' in item['generated_text']:  \n",
    "                answer_text += item['generated_text'].split('[/INST]')[1]\n",
    "        \n",
    "        outputs.add_as_json({\"answer\": answer_text})\n",
    "        return outputs\n",
    "    \n",
    "        # outputs.add_as_json([{\"generated_text\": s} for s in generated_text])\n",
    "        # return outputs\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Huggingface inference failed\")\n",
    "        # error handling\n",
    "        outputs = Output().error(str(e))\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "\n",
    "    return inference(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1ecec-79cf-4ed4-bba1-95e2fe79daea",
   "metadata": {},
   "source": [
    "### 3.2 Prepare the model metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c6e6828-bc70-4406-be01-364fbee214cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_ending = f'''engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.s3url = s3://{bucket}/{s3_model_prefix}/\n",
    "'''\n",
    "\n",
    "with open(f'{deploy_cache_location}/code/serving.properties', mode='w+') as file:\n",
    "    file.write(submission_ending)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717acac-e838-4785-8568-2eea9ee24329",
   "metadata": {},
   "source": [
    "### 3.3 Prepare the model depended python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38bf548e-fb01-4951-b49f-15a91c61fb2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../cache/meta-llama/Llama-2-7b-chat-hf/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $deploy_cache_location/code/requirements.txt\n",
    "-i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "transformers==4.28.1\n",
    "protobuf==3.20.1\n",
    "torch\n",
    "fairscale\n",
    "fire\n",
    "sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd01351-9308-4afa-b42f-86d02c59f0df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 Package all the model required resources and upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe41472-c2cf-4bb5-99aa-84df76c629b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/model.py\n",
      "code/serving.properties\n",
      "code/requirements.txt\n",
      "tar: code: file changed as we read it\n",
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-cn-northwest-1-768219110428/llm/code/meta-llama/Llama-2-7b-chat-hf/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm $deploy_cache_location/code/model.tar.gz\n",
    "!cd $deploy_cache_location/code && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf $deploy_cache_location/code/model.tar.gz -C $deploy_cache_location/ code\n",
    "\n",
    "s3_code_artifact = sess.upload_data(f\"{deploy_cache_location}/code/model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb01ed-6bd3-4880-a647-cfd71e692820",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: Start Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60d4dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = Model(image_uri=inference_image_uri,\n",
    "#               model_data=s3_code_artifact, \n",
    "#               role=role)\n",
    "\n",
    "# model.deploy(initial_instance_count = 1,\n",
    "#              instance_type = 'ml.p3.2xlarge', \n",
    "#              endpoint_name = endpoint_name,\n",
    "#              container_startup_health_check_timeout = 2900\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887f33f-2c3f-4ae0-8668-56a90cd7ca6d",
   "metadata": {},
   "source": [
    "### 4.1 Create Sagemaker Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6209d24-8473-4256-93d3-02e4e144386b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Model: arn:aws-cn:sagemaker:cn-northwest-1:768219110428:model/meta-llama-llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=endpoint_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff3a09-8e57-4940-baa8-001d317709b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 Create Sagemaker Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686abae8-5db7-4ebd-9fbf-5bd54f36c0ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws-cn:sagemaker:cn-northwest-1:768219110428:endpoint-config/meta-llama-llama-2-7b-chat-hf',\n",
       " 'ResponseMetadata': {'RequestId': '2f54c0cf-e96e-4572-9a29-f33b09bcd98d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2f54c0cf-e96e-4572-9a29-f33b09bcd98d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '118',\n",
       "   'date': 'Mon, 13 Nov 2023 09:53:50 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": endpoint_model_name,\n",
    "            \"InstanceType\": \"ml.p3.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0968fa-f6e8-467c-8b7c-6b2d64e6aa14",
   "metadata": {},
   "source": [
    "### 4.3 Create Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4c1df06-ae4a-42e2-9695-da0afa9ad734",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws-cn:sagemaker:cn-northwest-1:768219110428:endpoint/meta-llama-llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443df5c3-3ab6-46c3-b938-f4951f401f89",
   "metadata": {},
   "source": [
    "### 4.4 Monitor the Sagemaker Endpoint Creating Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c71240-6878-4fed-bf7d-2c1cf75f4ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws-cn:sagemaker:cn-northwest-1:768219110428:endpoint/meta-llama-llama-2-7b-chat-hf\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47bd4f0-dc3e-4471-a511-8646553694d3",
   "metadata": {},
   "source": [
    "## Step 5 : (Optional) Config Sagemaker Endpoint Autoscaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf889c90-7e59-4f33-87f9-ad700b7de98c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asg = boto3.client('application-autoscaling')\n",
    "\n",
    "# Resource type is variant and the unique identifier is the resource ID.\n",
    "resource_id=f\"endpoint/{endpoint_name}/variant/variant1\"\n",
    "\n",
    "# scaling configuration\n",
    "response = asg.register_scalable_target(\n",
    "    ServiceNamespace='sagemaker', #\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount', \n",
    "    MinCapacity=1,\n",
    "    MaxCapacity=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c81cfc1-98dd-4603-a4ef-4f8fcd9d8000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = asg.put_scaling_policy(\n",
    "    PolicyName=f'Request-ScalingPolicy-{endpoint_name}',\n",
    "    ServiceNamespace='sagemaker',\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    PolicyType='TargetTrackingScaling',\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        'TargetValue': 10.0, # Threshold\n",
    "        'PredefinedMetricSpecification': {\n",
    "            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n",
    "        },\n",
    "        'ScaleInCooldown': 300, # duration until scale in\n",
    "        'ScaleOutCooldown': 60 # duration between scale out\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddba20e-fc18-480d-9940-ae39695ac450",
   "metadata": {},
   "source": [
    "## Step 6: (Optional) Testing Model Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f1d98c-c107-4e7a-89e2-9ece350f5721",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.1 Prepare the Testing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f28db25-6996-440c-b004-14f96cfd982d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# endpoint_name = 'llama-2-model-2023-08-28-09-26-16-994'\n",
    "\n",
    "predictor = sagemaker.Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            sagemaker_session=sess,\n",
    "            serializer=serializers.JSONSerializer(),\n",
    "            deserializer=deserializers.JSONDeserializer(),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1b435-9b37-48e9-80e6-aeec897e6979",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.2 Demo 1: Generate the Embedding Value By Input Text Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52d4f56a-092e-4a6a-a920-48550ec9f20c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "\n",
      "\n",
      "<</SYS>>\n",
      "\n",
      "you should use the knowledge provided to answer user's question.  \n",
      "the knowledge you known are: [21] after modification.\n",
      "\n",
      "The ABTS+ radical reaction solution configuration was as follows: 5 mL of 7 mmol/L of ABTS and 5 mL of 2.45 mmol/L of potassium persulfate were mixed and stored in the dark for 12 h. Before use, 0.1 mol/L of pH 7.4 phosphate buffer saline (PBS) was added to dilute until the OD734 value was 0.70 ± 0.02.\n",
      "\n",
      "The sample solution was the same as that of the EPS sample solution measured by DPPH clearing ability.\n",
      "question: how to config the ABTS  radical reaction  ? [/INST]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': '  Based on the information provided, the ABTS+ radical reaction configuration is as follows:\\n\\n1. Volume: 5 mL\\n2. ABTS concentration: 7 mmol/L (5 mL x 7 mmol/L = 35 mmol ABTS)\\n3. Potassium persulfate concentration: 2.45 mmol/L (5 mL x 2.45 mmol/L = 12.25 mmol KPS)\\n4. Mixing time: 12 h (the reaction mixture is stored in the dark for 12 hours)\\n5. Dilution: After the reaction, 0.1 mol/L of pH 7.4 phosphate buffer saline (PBS) is added to dilute the solution until the OD734 value is 0.70 ± 0.02.\\n\\nSo, to configure the ABTS+ radical reaction, you will need:\\n\\n* 5 mL of 7 mmol/L ABTS solution\\n* 5 mL of 2.45 mmol/L potassium persulfate solution\\n* A mixing vessel (such as a beaker or flask)\\n* Darkness for the reaction to occur (12 hours)\\n* A spectrophotometer to measure the absorbance at 734'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "system_prompt = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "ask = \"\"\"\n",
    "you should use the knowledge provided to answer user's question.  \n",
    "the knowledge you known are: [21] after modification.\\n\\nThe ABTS+ radical reaction solution configuration was as follows: 5 mL of 7 mmol/L of ABTS and 5 mL of 2.45 mmol/L of potassium persulfate were mixed and stored in the dark for 12 h. Before use, 0.1 mol/L of pH 7.4 phosphate buffer saline (PBS) was added to dilute until the OD734 value was 0.70 ± 0.02.\\n\\nThe sample solution was the same as that of the EPS sample solution measured by DPPH clearing ability.\n",
    "question: how to config the ABTS  radical reaction  ? \n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(message: str, chat_history: list[tuple[str, str]]) -> str:\n",
    "    texts = [f'[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    for user_input, response in chat_history:\n",
    "        texts.append(f'{user_input.strip()} [/INST] {response.strip()} </s><s> [INST] ')\n",
    "    texts.append(f'{message.strip()} [/INST]')\n",
    "    return ''.join(texts)\n",
    "\n",
    "ask = get_prompt(ask, [])\n",
    "print(ask)\n",
    "\n",
    "predictor.predict(\n",
    "    {\"ask\": ask, \"parameters\": {\"max_new_tokens\": 300}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99727ada-9970-42c0-8f58-227109509dec",
   "metadata": {},
   "source": [
    "## 7: (Optional) Delete all resources (Sagemaker Model, Endpoint, Endpoint Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa0d6623-236d-4a5d-8360-4b07f8d8d40c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !aws sagemaker delete-endpoint-config --endpoint-config-name $endpoint_config_name\n",
    "# !aws sagemaker delete-endpoint --endpoint-name $endpoint_name\n",
    "# !aws sagemaker delete-model --model-name $endpoint_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded291b7-34cc-49b6-b9bf-87c17deed31b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
